import streamlit as st
import pandas as pd
import numpy as np
import re
import io

# ==========================================
# ãƒšãƒ¼ã‚¸è¨­å®š
# ==========================================
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“åˆ¤å®š", layout="wide")

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ åˆ¤å®šã‚¢ãƒ—ãƒª (Mobile Ver)")
st.write("Excelã¾ãŸã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

# ==========================================
# ãƒ­ã‚¸ãƒƒã‚¯é–¢æ•°ç¾¤
# ==========================================

def to_half_width(text):
    """å…¨è§’æ•°å­—ã‚’åŠè§’æ•°å­—ã«å¤‰æ›ã—ã€æ•°å­—ä»¥å¤–ã®æ–‡å­—ã‚’é™¤å»ã™ã‚‹"""
    if pd.isna(text): return text
    text = str(text)
    
    # 1. å…¨è§’æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    
    # 2. æ•°å­—ã¨ãƒ‰ãƒƒãƒˆä»¥å¤–ã‚’é™¤å» (ä¾‹: "11R" -> "11", "ç¬¬1ãƒ¬ãƒ¼ã‚¹" -> "1")
    # å°æ•°ç‚¹(å˜ã‚ªãƒƒã‚ºãªã©)ã‚‚è€ƒæ…®ã—ã¦ãƒ‰ãƒƒãƒˆã¯æ®‹ã™
    text = re.sub(r'[^\d\.]', '', text)
    
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

def load_and_clean_data(file_obj, filename, sheet_name=None):
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    if filename.lower().endswith('.csv'):
        try: df = pd.read_csv(file_obj, encoding='cp932', on_bad_lines='skip')
        except: df = pd.read_csv(file_obj, encoding='utf-8', on_bad_lines='skip')
    else:
        # Excelã®å ´åˆ
        if sheet_name:
            df = pd.read_excel(file_obj, sheet_name=sheet_name, engine='openpyxl')
        else:
            df = pd.read_excel(file_obj, engine='openpyxl')

    # åˆ—åã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° (ç©ºç™½é™¤å»)
    df.columns = df.columns.str.strip()
    
    # â˜…ãƒ˜ãƒƒãƒ€ãƒ¼åã®ã‚†ã‚‰ãå¸å (å…¨è§’Rã€ãƒ¬ãƒ¼ã‚¹è¡¨è¨˜ãªã©)
    rename_map = {
        'å ´æ‰€': 'å ´å', 
        'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 
        'èª¿æ•™å¸«': 'å©èˆ', 
        'ãƒ¬ãƒ¼ã‚¹': 'R',
        'ï¼²': 'R'  # å…¨è§’Rã«å¯¾å¿œ
    }
    df = df.rename(columns=rename_map)

    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'
    
    # â˜…æ•°å€¤åˆ—ã®å…¨è§’ãƒ»åŠè§’çµ±ä¸€ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            # å…¨è§’->åŠè§’å¤‰æ› & ä½™è¨ˆãªæ–‡å­—å‰Šé™¤
            df[col] = df[col].apply(to_half_width)
            # æ•°å€¤åŒ– (å¤‰æ›ã§ããªã„ã‚‚ã®ã¯NaNã«)
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Rã¨æ­£ç•ªãŒæœ‰åŠ¹ãªè¡Œã ã‘æ®‹ã™
    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int)
    df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    # åå‰æ­£è¦åŒ–
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns:
            df[col] = df[col].apply(normalize_name)
        else:
            df[col] = '' 

    # å¿…è¦ãªåˆ—ã®ç¢ºä¿
    potential_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in potential_cols:
        if col not in df.columns: df[col] = np.nan
            
    return df[potential_cols].copy()

def calc_haichi_numbers(df: pd.DataFrame) -> pd.DataFrame:
    # æ—¢å­˜ã®å€¤ãŒã‚ã‚Œã°å„ªå…ˆ
    if df[['é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']].notna().all().all():
        df['è¨ˆç®—_é€†ç•ª'] = df['é€†ç•ª']
        df['è¨ˆç®—_æ­£å¾ªç’°'] = df['æ­£å¾ªç’°']
        df['è¨ˆç®—_é€†å¾ªç’°'] = df['é€†å¾ªç’°']
        df['é ­æ•°'] = df['é ­æ•°'] if 'é ­æ•°' in df.columns else 16
        return df

    # é ­æ•°è¨ˆç®—
    if 'é ­æ•°' in df.columns and df['é ­æ•°'].notna().any():
        df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(16).astype(int)
    else:
        race_counts = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].max().to_dict()
        df['ä½¿ç”¨é ­æ•°'] = df.apply(lambda x: race_counts.get((x['å ´å'], x['R']), 16), axis=1)

    def calc_row(row):
        total = int(row['ä½¿ç”¨é ­æ•°'])
        seiban = int(row['æ­£ç•ª'])
        gyakuban = int(row['é€†ç•ª']) if pd.notna(row['é€†ç•ª']) else (total + 1) - seiban
        sei_j = int(row['æ­£å¾ªç’°']) if pd.notna(row['æ­£å¾ªç’°']) else total + seiban
        gyaku_j = int(row['é€†å¾ªç’°']) if pd.notna(row['é€†å¾ªç’°']) else total + gyakuban
        return pd.Series([total, gyakuban, sei_j, gyaku_j])
    
    df[['é ­æ•°', 'è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc_row, axis=1)
    return df

def get_pair_pattern(row1, row2):
    def val(x):
        try: return int(float(x)) 
        except: return None
    r1 = [val(row1.get('æ­£ç•ª')), val(row1.get('è¨ˆç®—_é€†ç•ª')), val(row1.get('è¨ˆç®—_æ­£å¾ªç’°')), val(row1.get('è¨ˆç®—_é€†å¾ªç’°'))]
    r2 = [val(row2.get('æ­£ç•ª')), val(row2.get('è¨ˆç®—_é€†ç•ª')), val(row2.get('è¨ˆç®—_æ­£å¾ªç’°')), val(row2.get('è¨ˆç®—_é€†å¾ªç’°'))]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i * 4 + j] for i in range(4) for j in range(4)
             if r1[i] is not None and r2[j] is not None and r1[i] == r2[j] and r1[i] != 0]
    b1, b2 = val(row1.get('æ­£ç•ª')), val(row2.get('æ­£ç•ª'))
    if b1 is not None and b2 is not None:
        if str(b1)[-1] == str(b2)[-1]:
            if b1 < 10 and b2 >= 10: pairs.append('Q')
            elif b1 >= 10 and b2 < 10: pairs.append('R')
    return ",".join(pairs)

def get_common_values(group: pd.DataFrame):
    cols = ['æ­£ç•ª', 'è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']
    common_set = None
    for _, row in group.iterrows():
        current_set = set()
        for col in cols:
            val = row.get(col)
            if pd.notna(val):
                try:
                    num = int(float(val))
                    if num != 0: current_set.add(num)
                except: continue
        if common_set is None: common_set = current_set
        else: common_set = common_set.intersection(current_set)
        if not common_set: return None
    if common_set: return ','.join(map(str, sorted(list(common_set))))
    return None

def find_all_pairs(df: pd.DataFrame) -> pd.DataFrame:
    all_pairs = []
    df = df.sort_values(by=['R', 'å ´å']).reset_index(drop=True)
    # é¨æ‰‹
    for name, group in df.groupby('é¨æ‰‹'):
        if name == "": continue
        group = group.sort_values('R').to_dict('records')
        for i in range(len(group) - 1):
            curr, next_r = group[i], group[i+1]
            if curr['å ´å'] != next_r['å ´å']: continue
            detected = get_pair_pattern(curr, next_r)
            if detected:
                all_pairs.append({'å ´å': curr['å ´å'], 'å¯¾è±¡å': name, 'å±æ€§': 'é¨æ‰‹', 'ãƒ¬ãƒ¼ã‚¹A': curr['R'], 'é¦¬åA': curr['é¦¬å'], 'ãƒ¬ãƒ¼ã‚¹B': next_r['R'], 'é¦¬åB': next_r['é¦¬å'], 'ãƒ‘ã‚¿ãƒ¼ãƒ³': detected, 'ç·å‡ºèµ°æ•°': len(group)})
    # å©èˆ
    if 'å©èˆ' in df.columns:
        for (place, name), group in df.groupby(['å ´å', 'å©èˆ']):
            if name == "": continue
            races = group.sort_values('R').to_dict('records')
            for i in range(len(races)):
                for j in range(i + 1, len(races)):
                    curr, next_r = races[i], races[j]
                    detected = get_pair_pattern(curr, next_r)
                    if detected:
                        all_pairs.append({'å ´å': place, 'å¯¾è±¡å': name, 'å±æ€§': 'å©èˆ', 'ãƒ¬ãƒ¼ã‚¹A': curr['R'], 'é¦¬åA': curr['é¦¬å'], 'ãƒ¬ãƒ¼ã‚¹B': next_r['R'], 'é¦¬åB': next_r['é¦¬å'], 'ãƒ‘ã‚¿ãƒ¼ãƒ³': detected, 'ç·å‡ºèµ°æ•°': len(races)})
    # é¦¬ä¸»
    if 'é¦¬ä¸»' in df.columns:
        for name, group in df.groupby('é¦¬ä¸»'):
            if name == "": continue
            races = group.sort_values(['R', 'å ´å']).to_dict('records')
            for i in range(len(races)):
                for j in range(i + 1, len(races)):
                    curr, next_r = races[i], races[j]
                    detected = get_pair_pattern(curr, next_r)
                    if detected:
                        loc = f"{curr['å ´å']}â†’{next_r['å ´å']}" if curr['å ´å'] != next_r['å ´å'] else curr['å ´å']
                        all_pairs.append({'å ´å': loc, 'å¯¾è±¡å': name, 'å±æ€§': 'é¦¬ä¸»', 'ãƒ¬ãƒ¼ã‚¹A': curr['R'], 'é¦¬åA': curr['é¦¬å'], 'ãƒ¬ãƒ¼ã‚¹B': next_r['R'], 'é¦¬åB': next_r['é¦¬å'], 'ãƒ‘ã‚¿ãƒ¼ãƒ³': detected, 'ç·å‡ºèµ°æ•°': len(races)})
    return pd.DataFrame(all_pairs)

def get_blue_recommendations(df_calculated: pd.DataFrame) -> pd.DataFrame:
    blue_recs = []
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col not in df_calculated.columns: continue
        group_keys = ['å ´å', col] if col == 'é¨æ‰‹' else [col]
        try:
            for name, group in df_calculated.groupby(group_keys):
                target_name = None
                if len(group_keys) == 2:
                    if isinstance(name, tuple) and len(name) == 2: location, target_name = name
                else:
                    if isinstance(name, tuple) and len(name) == 1: target_name = name[0]
                    elif isinstance(name, str): target_name = name
                
                if not target_name or target_name == "" or len(group) < 2: continue
                common_vals = get_common_values(group)
                if common_vals:
                    remark = f'{col}å…±é€šå€¤ ({common_vals})'
                    if len(group) == 2: remark += " (2éé™å®š)"
                    for _, row in group.iterrows():
                        blue_recs.append({'å ´å': row['å ´å'], 'R': row['R'], 'é¦¬å': row['é¦¬å'], 'å±æ€§': col, 'å¯¾è±¡å': target_name, 'åˆ¤å®š': 'â˜… é’å¡—å¯¾è±¡', 'æ¡ä»¶': remark, 'é‡è¦åº¦': 9})
        except: continue
    df_blue = pd.DataFrame(blue_recs)
    if not df_blue.empty:
        df_blue = df_blue.drop_duplicates(subset=['å ´å', 'R', 'é¦¬å', 'åˆ¤å®š'], keep='last')
        df_blue = pd.merge(df_blue, df_calculated[['å ´å', 'R', 'é¦¬å', 'å˜ï½µï½¯ï½½ï¾']], on=['å ´å', 'R', 'é¦¬å'], how='left')
        return df_blue
    return pd.DataFrame()

def evaluate_and_score(df_pairs: pd.DataFrame, df_original_data: pd.DataFrame) -> pd.DataFrame:
    recommendations = []
    high_prob_patterns = ['C', 'D', 'G', 'H']
    for _, row in df_pairs.iterrows():
        race_a, race_b = row['ãƒ¬ãƒ¼ã‚¹A'], row['ãƒ¬ãƒ¼ã‚¹B']
        horse_a, horse_b = row['é¦¬åA'], row['é¦¬åB']
        target_name, pattern = row['å¯¾è±¡å'], row['ãƒ‘ã‚¿ãƒ¼ãƒ³']
        place_name, attribute = row['å ´å'], row['å±æ€§']
        is_blue = (row['ç·å‡ºèµ°æ•°'] == 2 and attribute == 'é¨æ‰‹')
        
        jb, pb = ("â— ç‹™ã„ç›®", 3)
        if is_blue: jb, pb = ("â˜† 2éãƒšã‚¢", 5)
        elif any(p in pattern for p in high_prob_patterns): jb, pb = ("â—‹ ãƒãƒ£ãƒ³ã‚¹", 4)
        recommendations.append({'å ´å': place_name, 'R': race_b, 'é¦¬å': horse_b, 'é¨æ‰‹/å©èˆ/é¦¬ä¸»': f"{attribute}:{target_name}", 'åˆ¤å®š': jb, 'æ¡ä»¶': f"ãƒšã‚¢({race_a}R {horse_a})å‡¡èµ°å¾…ã¡/ãƒ‘ã‚¿ãƒ¼ãƒ³:{pattern}", 'é‡è¦åº¦': pb})
        
        ja, pa = ('â–² å…ˆè²·ã„ãƒªã‚¹ã‚¯', 1) if not is_blue else ('â—‹ 2éå…ˆè²·ã„', 2)
        recommendations.append({'å ´å': place_name, 'R': race_a, 'é¦¬å': horse_a, 'é¨æ‰‹/å©èˆ/é¦¬ä¸»': f"{attribute}:{target_name}", 'åˆ¤å®š': ja, 'æ¡ä»¶': f"æ¬¡èµ°{race_b}Rã«ãƒšã‚¢ã‚ã‚Š/ãƒ‘ã‚¿ãƒ¼ãƒ³:{pattern}", 'é‡è¦åº¦': pa})
    
    df_rec = pd.DataFrame(recommendations)
    if df_rec.empty: return pd.DataFrame()
    df_rec = pd.merge(df_rec, df_original_data[['R', 'é¦¬å', 'å˜ï½µï½¯ï½½ï¾']], on=['R', 'é¦¬å'], how='left')
    
    final_list = []
    for _, row in df_rec.iterrows():
        odds = row['å˜ï½µï½¯ï½½ï¾']
        priority = row['é‡è¦åº¦']
        if pd.isna(odds): pass
        elif odds > 49.9:
            if priority >= 3:
                row['åˆ¤å®š'] = 'â–³ ç´å€™è£œ'
                row['é‡è¦åº¦'] = 0
                row['æ¡ä»¶'] = f'ã€é«˜é…ã€‘' + row['æ¡ä»¶']
            else: continue 
        elif 10.0 <= odds <= 20.0 and priority >= 3:
            row['åˆ¤å®š'] = row['åˆ¤å®š'].replace('ç‹™ã„ç›®', 'ç‹™ã„ç›®(é«˜)')
            row['é‡è¦åº¦'] += 1
        final_list.append(row)
    return pd.DataFrame(final_list)

# ==========================================
# ãƒ¡ã‚¤ãƒ³ UI å‡¦ç†
# ==========================================

uploaded_file = st.file_uploader("", type=['xlsx', 'xlsm', 'csv'])

if uploaded_file is not None:
    sheet_name = None
    if uploaded_file.name.endswith(('.xlsx', '.xlsm')):
        try:
            xl = pd.ExcelFile(uploaded_file, engine='openpyxl')
            sheet_list = xl.sheet_names
            if len(sheet_list) > 1:
                sheet_name = st.selectbox("ã‚·ãƒ¼ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„", sheet_list)
            else:
                sheet_name = sheet_list[0]
        except Exception as e:
            st.error(f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    if st.button('åˆ¤å®šå®Ÿè¡Œ'):
        with st.spinner('åˆ†æä¸­...'):
            try:
                uploaded_file.seek(0)
                df_all = load_and_clean_data(uploaded_file, uploaded_file.name, sheet_name)
                
                if df_all.empty:
                    st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    df_calculated = calc_haichi_numbers(df_all.copy())
                    
                    df_all_pairs = find_all_pairs(df_calculated)
                    df_blue = get_blue_recommendations(df_calculated)
                    df_ar = evaluate_and_score(df_all_pairs, df_all)
                    
                    if not df_blue.empty:
                        df_blue = df_blue.rename(columns={'å¯¾è±¡å': 'é¨æ‰‹/å©èˆ/é¦¬ä¸»'})
                        df_blue = df_blue.assign(**{'é¨æ‰‹/å©èˆ/é¦¬ä¸»': lambda x: x['å±æ€§'] + ':' + x['é¨æ‰‹/å©èˆ/é¦¬ä¸»']}).drop(columns=['å±æ€§'])
                        df_final = pd.concat([df_blue, df_ar], ignore_index=True)
                    else:
                        df_final = df_ar

                    if df_final.empty:
                        st.info("æ¨å¥¨é¦¬ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        # é‡è¤‡ã¾ã¨ã‚
                        df_final = df_final.sort_values('é‡è¦åº¦', ascending=False)
                        agg_rules = {
                            'é¨æ‰‹/å©èˆ/é¦¬ä¸»': lambda x: ' + '.join(sorted(set(x))), 
                            'å˜ï½µï½¯ï½½ï¾': 'first',
                            'åˆ¤å®š': 'first',
                            'æ¡ä»¶': lambda x: ' / '.join(x),
                            'é‡è¦åº¦': 'sum'
                        }
                        df_final = df_final.groupby(['å ´å', 'R', 'é¦¬å'], as_index=False).agg(agg_rules)
                        # ã‚½ãƒ¼ãƒˆ: å ´å > ãƒ¬ãƒ¼ã‚¹ > é‡è¦åº¦
                        df_final = df_final.sort_values(['å ´å', 'R', 'é‡è¦åº¦'], ascending=[True, True, False])

                        st.success("åˆ†æå®Œäº†ï¼")
                        
                        cols = ['å ´å', 'R', 'é¦¬å', 'é¨æ‰‹/å©èˆ/é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'åˆ¤å®š', 'æ¡ä»¶']
                        
                        buffer = io.BytesIO()
                        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                            df_final.to_excel(writer, index=False, sheet_name='çµæœ')
                        
                        st.download_button(
                            label="ğŸ’¾ çµæœã‚’Excelã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=buffer.getvalue(),
                            file_name="result.xlsx",
                            mime="application/vnd.ms-excel"
                        )
                        st.dataframe(df_final[cols], hide_index=True, use_container_width=True)

            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
